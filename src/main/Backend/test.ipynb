{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_PATH = os.path.abspath(\"../../var/database\")\n",
    "DOCS_PATH = os.path.abspath(\"../../docs\")\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# Read model name\n",
    "with open(os.path.join(DATABASE_PATH, \"model.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    EMBEDDINGS_MODEL_NAME = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding():\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings_model = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDINGS_MODEL_NAME,\n",
    "        model_kwargs={\"device\": DEVICE}\n",
    "    )\n",
    "    vector_store = FAISS.load_local(\n",
    "        DATABASE_PATH,\n",
    "        embeddings=embeddings_model,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    return embeddings_model, vector_store\n",
    "\n",
    "\n",
    "embeddings_model, vector_store = get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    base_url=\"http://database_ollama:11434\",\n",
    "    model=\"qwen2.5:1.5b\",\n",
    "    temperature = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_docs(questions):\n",
    "    \n",
    "    \"\"\"\n",
    "    Find documents by questions\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = np.array(embeddings_model.embed_documents(questions))\n",
    "    mean_embedding = embeddings.mean(axis=0).tolist()\n",
    "    docs = vector_store.similarity_search_by_vector(mean_embedding, k=3)\n",
    "    return embeddings, docs\n",
    "\n",
    "def print_docs(docs):\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"Result {i+1}:\")\n",
    "        print(f\"ID: {doc.metadata['id']}\")\n",
    "        print(f\"Content:\\n{doc.page_content}\")\n",
    "        #print(f\"Distance: {distance}\")\n",
    "        if i + 1 < len(docs):\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def get_llm_question(llm, question, history):\n",
    "        \n",
    "    prompt = [\n",
    "        (\"system\",\n",
    "            \"Ты помощник, который делает вопросы самодостаточными, добавляя контекст из истории диалога\"\n",
    "        ),\n",
    "        (\"human\",\n",
    "            f\"История диалога:\\n\\n\" + \"\\n\".join(history) + \"\\n\\n\" +\n",
    "            f\"Переформулируй вопрос, на основе истории диалога, \" +\n",
    "                f\"чтобы он был понятен без контекста: {question}. Вопрос:\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    print(str(prompt) + \"\\n\")\n",
    "    \n",
    "    answer = llm.invoke(prompt).content\n",
    "    \n",
    "    print(\"Новый вопрос: \" + answer + \"\\n\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "def get_context1(question, history):\n",
    "    questions = history + [question]\n",
    "    \n",
    "    # Ищем релевантные документы\n",
    "    _, docs = find_docs(questions)\n",
    "    print_docs(docs)\n",
    "\n",
    "\n",
    "def get_context2(question, history):\n",
    "    \n",
    "    new_question = question\n",
    "    \n",
    "    # Переформулировка вопроса\n",
    "    if len(history) > 0:\n",
    "        new_question = get_llm_question(llm, question, history)\n",
    "    \n",
    "    # Ищем релевантные документы\n",
    "    _, docs = find_docs([new_question])\n",
    "    print_docs(docs)\n",
    "\n",
    "\n",
    "history = [\n",
    "    \"Что такое BayLang?\"\n",
    "]\n",
    "question = \"Как его установить?\"\n",
    "get_context2(question, history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

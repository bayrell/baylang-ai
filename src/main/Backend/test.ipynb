{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from make_index import MarkdownParser\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from sentence_transformers import CrossEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Автоперезагрузка файлов\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_PATH = os.path.abspath(\"../../var/database\")\n",
    "DOCS_PATH = os.path.abspath(\"../../docs\")\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# Read model name\n",
    "with open(os.path.join(DATABASE_PATH, \"model.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    EMBEDDINGS_MODEL_NAME = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding():\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings_model = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDINGS_MODEL_NAME,\n",
    "        model_kwargs={\"device\": DEVICE}\n",
    "    )\n",
    "    vector_store = FAISS.load_local(\n",
    "        DATABASE_PATH,\n",
    "        embeddings=embeddings_model,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    return embeddings_model, vector_store\n",
    "\n",
    "\n",
    "embeddings_model, vector_store = get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    base_url=\"http://database_ollama:11434\",\n",
    "    model=\"qwen2.5:1.5b\",\n",
    "    temperature = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "#reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "#reranker = CrossEncoder('cross-encoder/stsb-TinyBERT-L-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_docs(questions, k=3):\n",
    "    \n",
    "    \"\"\"\n",
    "    Find documents by questions\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = np.array(embeddings_model.embed_documents(questions)).astype(np.float32)\n",
    "    mean_embedding = embeddings.mean(axis=0)\n",
    "    faiss.normalize_L2(mean_embedding.reshape(1, -1))\n",
    "    docs = vector_store.similarity_search_by_vector(mean_embedding.tolist(), k)\n",
    "    return embeddings, docs\n",
    "\n",
    "\n",
    "def find_docs2(question, k=3):\n",
    "    \n",
    "    \"\"\"\n",
    "    Find documents by questions\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = np.array(embeddings_model.embed_query(question)).astype(np.float32)\n",
    "    faiss.normalize_L2(embeddings.reshape(1, -1))\n",
    "    docs = vector_store.similarity_search_by_vector(embeddings.tolist(), k)\n",
    "    return embeddings, docs\n",
    "\n",
    "\n",
    "def print_docs(docs):\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"Result {i+1}\")\n",
    "        print(f\"ID: {doc.metadata['id']}\")\n",
    "        print(f\"Content:\\n{doc.page_content}\")\n",
    "        #print(f\"Distance: {distance}\")\n",
    "        if i + 1 < len(docs):\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def get_llm_question(llm, question, history):\n",
    "        \n",
    "    prompt = [\n",
    "        (\"system\",\n",
    "            \"Ты помощник, который делает вопросы самодостаточными, добавляя контекст из истории диалога\"\n",
    "        ),\n",
    "        (\"human\",\n",
    "            f\"История диалога:\\n\\n\" + \"\\n\".join(history) + \"\\n\\n\" +\n",
    "            f\"Переформулируй вопрос, на основе истории диалога, \" +\n",
    "                f\"чтобы он был понятен без контекста: {question}. Вопрос:\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    print(str(prompt) + \"\\n\")\n",
    "    \n",
    "    answer = llm.invoke(prompt).content\n",
    "    \n",
    "    print(\"Новый вопрос: \" + answer + \"\\n\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "def get_context1(question, history, k=3):\n",
    "    questions = history + [question]\n",
    "    \n",
    "    # Ищем релевантные документы\n",
    "    _, docs = find_docs(questions, k)\n",
    "    return docs\n",
    "\n",
    "\n",
    "def get_context2(question, history, k=3):\n",
    "    \n",
    "    new_question = question\n",
    "    \n",
    "    # Переформулировка вопроса\n",
    "    if len(history) > 0:\n",
    "        new_question = get_llm_question(llm, question, history)\n",
    "    \n",
    "    # Ищем релевантные документы\n",
    "    _, docs = find_docs([new_question], k)\n",
    "    return docs\n",
    "\n",
    "\n",
    "def rerank_docs(query, docs, k=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Переранжирует документы с помощью cross-encoder.\n",
    "    \"\"\"\n",
    "    \n",
    "    pairs = [(query, doc.page_content) for doc in docs]\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    # Сортируем по убыванию релевантности\n",
    "    ranked_docs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    ranked_docs = [doc for doc, score in ranked_docs]\n",
    "    return ranked_docs[0:k]\n",
    "\n",
    "\n",
    "history = [\n",
    "    \"Что такое облачная BAYRELL Cloud OS?\"\n",
    "]\n",
    "question = \"Как его установить на сервер?\"\n",
    "docs = get_context2(question, history, k=5)\n",
    "#docs = rerank_docs(question, docs, k=3)\n",
    "print_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markdown_test():\n",
    "    file_name = os.path.join(DOCS_PATH, \"ru/cloud-os/install-desktop.md\")\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        parser = MarkdownParser()\n",
    "        parser.parse(f.read())\n",
    "        for i, block in enumerate(parser.blocks):\n",
    "            print(f\"Result {i+1}\")\n",
    "            print(block[\"content\"])\n",
    "            if i + 1 < len(parser.blocks):\n",
    "                print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запрос и кандидаты\n",
    "query = \"Что такое Облачная ОС?\"\n",
    "documents = [\n",
    "    \"Облачная ОС — это операционная система, работающая в облаке.\",\n",
    "    \"Компьютерная сеть — это соединение устройств для обмена данными.\",\n",
    "    \"Linux — это операционная система с открытым кодом.\"\n",
    "]\n",
    "\n",
    "# Формируем пары (query, document)\n",
    "pairs = [(query, doc) for doc in documents]\n",
    "\n",
    "# Предсказываем релевантность (чем выше — тем лучше)\n",
    "scores = reranker.predict(pairs)\n",
    "\n",
    "# Сортируем документы по убыванию оценки\n",
    "ranked_docs = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Выводим результаты\n",
    "for i, (doc, score) in enumerate(ranked_docs):\n",
    "    print(f\"Rank {i+1}: {doc} (Score: {score:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
